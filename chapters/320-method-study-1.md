# Methodology

The literature points to an urgent need for research around how individuals and groups may benefit from a pedagogical design and environment that provides affordances for socially shared regulation for learning (SSRL) and productive interactions leading to shared knowledge object change, as both have been shown to improve the quality of collaborative products (knowledge objects), and by inference, individual and shared cognitive and metacognitive strategies. Computer-supported collaborative learning (CSCL) environments are thought particularly suited to this area of research because they can be designed to be instrumentalisable by users in response to problem-solving tasks that are conducted over time, in both synchronous and asynchronous modes. The CSCL environment also functions as a mediating layer through which both social and cognitive processes occur, and it can contain useful epistemic and regulative resources. Importantly for the emerging fields of Learning Analytics (LA) and Natural Language Processing (NLP) by Artificial Intelligence (AI) systems, it can also provide data on what occurs within the environment and with these resources. This means we can have some insight into whether our designed environment is used as it intended or, if modified, the way in which it has been changed. We can also see whether this changes after groups create and share a knowledge object designed to be helpful in all learners&#39; epistemic and regulative development; and, again, after they receive one.

The setting for this contribution to this research is the University of Sydney, initially in two Masters&#39; level Learning Sciences units and one first year Engineering unit in which learners collaborate on system model design over the course of the semester as an ordinary part of the learning outcomes.

Initially, the research questions are: [in need of update with new insights into data forms]

1. How do groups create and develop knowledge objects for (a) modelling and (b) group regulation purposes?
  1. How do groups appropriate designer-provided notations, tools, roles and workflows? (First level appropriation)
2. How does the expectation to have to communicate insights and practices regarding (a) and (b) to another group—the &#39;target group&#39;—affect knowledge building of the &#39;source group&#39;? (&quot;Production effect&quot;)
3. How does the information on (a) and (b) made available from another group—the source group—affect the target group? (&quot;Variation effect&quot;)
  1. How do groups appropriate tools and[language- can we consider constructs epistemic resources] workflows/models? provided by another group? (Second level appropriation).

## Design

This project approached the research questions though the lens of Design-Based Research (DBR) as its overarching aim is to make the research about learning relevant for learning settings {Reimann, 2011 #6637}. DBR offers the advantage of affording both quantitative and qualitative approaches, which allows quantitative data analysis while still comprising rich depth of qualitative inquiry. It is considered an appropriate approach for an authentic setting, in close cooperation with instructors and students as co-designers of the learning experience {Penuel, 2018 #7107}.

The three studies reported here are quite imperfect, taking place as they do in a global higher education context, with students of diverse ages, backgrounds and levels of technical and higher education expertise. The participants are studying in an organisation that has both competitive and political context in Australia and internationally, subject to management imperatives and global pandemics. Each study is reported on individually, as their unique circumstances mean they are not directly comparable. What is common to all is that they rely on a web of actors and technology, pedagogy necessarily responding to circumstance, and those circumstances sometimes out of our control. This is the power of DBR, to respond and adapt to change as it occurs. Not always ahead of the game but resourceful and ready for the challenges that are inevitable in our complex and interdependent world. But which aspects of the design are most instrumentalisable? And what does knowing this mean for the research questions?

There is substantial evidence that Group Model Building (GMB) helps participants to align their mental models, create agreement and consensus, and generate commitment to a modelled solution of a complex systemic problem {Vennix, 1996 #2389;Andersen, 1997 #2372;Richardson, 1995 #7683;Stave, 2002 #2371}. This makes it particularly suited to our context of collaborative design of systems, using model form and notation. However, it has not been demonstrated that people&#39;s mental models have changed through the activity {Andersen, 1997 #2370@188}. One explanation for this is that the synchronous event-based modes that are traditionally used for modelling workshops do not allow participants to co-construct their knowledge over time. Another explanation is that the expert facilitator inhibits the development of systems thinking knowledge, as the expert enacts the materialisation of concepts into model notation and form, instead of participants negotiating the embodied process. There is a need for more research on how people who are not specialists in model building (novice modellers) can benefit from the process {Hoppenbrouwers, 2009 #2269}. This environment and task are designed to give the novice participants experience in co-constructing models, taking turns in the modelling role, working with other team members to develop a shared understanding of the system that incorporates diverse perspectives.

While {Rabardel, 2003 #9085;Rabardel, 2005 #7690} theory of instrumental genesis is not specifically about learning, this approach can be used in CSCL at macro and micro levels. Users who engage in the dialectic between subject and instrument gain competencies in collaboration and meta-skills development, but there is a need for research on practical ways to evaluate user-instrumentalisable systems {Lonchamp, 2012 #7681}. Both Rabardel &amp; Beguin&#39;s (2005) principles of instrument-mediated activity and {Paavola, 2014 #9179@@author-year} design principles for trialogical learning emphasise the role of user creativity and inventiveness in appropriating and transforming CSCL environments to externalise and materialise their individual and shared knowledge. An associated principle is that of &quot;cross fertilization&quot; of practices and artefacts across communities and institutions {Paavola, 2014 #9179@5}, designing for instrumental genesis in an iterative and ongoing dialogue between designers and users {Rabardel, 2005 #7690}. Appropriation can take place at multiple levels, and this research will contribute to the evolving design of CSCL collaboration environments by analysing the ways in which learners instrumentalise the designed environment at the first level, and the artefacts provided by group and peers at the second level.

We have also seen from {Damşa, 2016 #9089@@author-year} and {Paavola, 2014 #9179@@author-year} that frequency of iterative and productive interactions is important for high quality knowledge object development, and from {Panadero, 2015 #9212} that the process of SSRL unfolds over time. These interactions are based on elaborative epistemic actions like discussion around meaning-making, plus regulative and productional activities where the knowledge object undergoes related change {Damşa, 2010 #1891;Damşa, 2014 #1098;Damşa, 2016 #9089}. More research is needed in learning situations that are not single-event, but which offer time {Damşa, 2014 #1098} and explicit resources for elaboration of knowledge into material forms that may then be shared and taken up (or not) into the knowledge objects through deliberate consideration {Damşa, 2010 #1891}. The outcomes of these processes can be observed through evaluating the quality of the knowledge objects over time, and the relationship of their evolution to the co-construction process. Socially shared regulation for learning takes time because it needs consistent efforts to regulate both learning and engagement {Järvelä, 2011 #9225}. The role that CSCL environments play in developing SSRL and how we might design for them is also a significant gap in current research {Panadero, 2015 #9212;Järvelä, 2013 #9224}. As the features of SSRL are related to and in some cases overlap with the regulative dimensions identified by {Damşa, 2016 #9089} they should be incorporated into this study as regulated learning is considered the quintessential skill for collaboration {Järvelä, 2013 #9224}.

An overview of design conjectures is at Table 1 in the Literature Review.

## Study 1

### Participants and context

The participants ([_N_=8, 7 females, 1 male] or [_N_=17, 14 females, 3 males) were enrolled students in a postgraduate Learning Sciences unit that engages learners in instructional and other types of design and modelling. Participants were verbally advised about the study and its purpose, and provided with both Participant Information Statement (Appendix [A]) and Participant Consent Form (Appendix [A]) which explained the study and their options for consent, as well as the data that would be collected and how it would be used. The population is not considered vulnerable as it comprises postgraduate, generally adult, students who are accustomed to the communication channels usually used by the University, such as Canvas, and whose levels of language competence are appropriate to tertiary study. [Of that eight, four surveys were lost in an unplanned system outage during completion, resulting in N=4, 3 females and 1 male. All four also completed the reflection task at the end of the unit of study.][Of that 17, one participant did not complete the survey, one participant discontinued the unit prior to completion, and eight surveys were lost in an unplanned system outage during completion. Fifteen participants also completed the reflection task at the end of the unit of study.]

Participation in the study was voluntary, and neither instructor nor researcher knew which students were participating until all assessable work was returned and administrative processes complete. The group project task was cross-graded by another Learning Sciences academic to ensure even unintentional bias was avoided.

The sample is consistent in size and composition with {Damşa, 2016 #7100}&#39;s Teaching Education Professional Environments cohort and the Learning and Educational Technology students participating in the study by {Järvelä, 2013 #7115}, and is typical for a unit of this kind.

During the study, the students collaborated in two phases, firstly in triples in an ungraded research task, and then in groups of six in a graded group project. Triple formation was randomised, with all names being added to a spinning wheel ([wheeldecide.com](https://wheeldecide.com/index.php?c1=Aaron+Dugdale&amp;c2=Agustina+Salas+Loydi&amp;c3=Angela-Claire+Mcdonald&amp;c4=Courtney+Frost&amp;c5=Daniela+Mansilla+Jamett&amp;c6=Diego+Sanchez&amp;c7=Dominique+Cama&amp;c8=Juli+Ormeno&amp;c9=Kavita+Poojari&amp;c10=Kira+Thomas-Schumacher&amp;c11=Lauren+Cole&amp;c12=Lucy+McLoughlin&amp;c13=Madeline+Barnes&amp;c14=Matt+Nelson&amp;c15=Michelle+Durkan&amp;c16=Ongprang+Sangsurin&amp;c17=Paola+Ceriola&amp;c18=Quang+Le&amp;c19=Samadhi+Driscoll&amp;c20=Tahlia+Glen&amp;c21=Wendy+Newton&amp;col=pastel&amp;t=Chocolate+Pairs%21&amp;time=5&amp;width=500)) and grouped in the order in which they were &quot;drawn&quot;. Once a name had been drawn it was removed from the wheel. The purpose of the triple task was to familiarise the cohort with Markdown language, which was important to the subsequent group task, and with initial collaboration in a low-stakes activity. The output from the triple task was a collaborative analysis of &quot;state of the art&quot; resources, specifically design principles, patterns and models, relevant to learning designers&#39; professional development.

Three groups of six were then formed (one group member declined to take part in this study and will not be reported). Two groups comprised two original triples, with the third combining the remaining members of triples where some members hadn&#39;t continued past Census date. A GitHub repo was instantiated for each group, and all the triple task reports added to each repo, as the first step in the knowledge exchange.

\&lt;limitations?\&gt;One unforeseen issue was a Census date nearly half way through the taught unit, which meant that the group task was already under way when several students discontinued the unit. Another was a delay in Ethics approval, meaning that surveys could not be conducted as early in the session as planned; an IT restructure meant a dedicated server was not ready in time for the survey, and other university-hosted mirror websites were unexpectedly decommissioned during the study. In the end, the IT services were not available until four months after approval, something that could not reasonably be foreseen.] \&lt;/limitations?\&gt;

### Collaborative context

Over seven weeks (Week 6 to Week 13 of the teaching session), the groups of six worked in a team-based, intentionally-designed CSCL environment to co-construct a meta-design in the PlantUML modelling language representing a &quot;future learning environment&quot; no more than five years from now. Each class included around 20 minutes of skill building in PlantUML, with students following worked examples which were demonstrated by the instructor. Outside class, small ungraded weekly tasks using PlantUML were set for groups with the explicit aim of creating increasingly sophisticated models for both collaborative process and domain understanding.

Students also completed individual tasks over the session; those and the ungraded triple task shown in light grey at Table [T].

\&lt;inclusion?\&gt;

Should I be more specific about this? Or should I remove everything except the two KE weeks?]

\&lt;/inclusion?\&gt;

_Table 2: Study 1 Student activities over the teaching session._

| Teaching week | Focus | Student activities |
| --- | --- | --- |
| Weeks 2-12 | Individual domain reading topics | Presentation in class |
| Week 4 | Triple formation | Begin task to find and model key features of a community site |
| Week 6 | Triple presentations | Present initial findings to others in class time |
| Week 6 | Group formation
 Knowledge Exchange 1 – triple task | Begin task to model current, achievable and future states of a learning design environment relevant to group interests |
| Weeks 6-12 | Group activities | Draft model and evaluate within group |
| Week 11 | Group repo available to others
**Knowledge Exchange – team models** | Refine model/s |
| Week 12 | Individual reading topics | Essay submission |
| Week 12 | Group presentation | Present final model to others in class time |
| Week 13 | Individual reflection | Reflection submission |

### Designed environment

The designed environment comprised a combination of platforms and tools designed to be user-instrumentalisable by learners in their epistemic, regulative and social collaboration. Table [T] sets out the initial design [green], and also features which were added to the environment when a need was identified [purple].

_Table 3: Designed environment features in detail._

| Type | Purpose | Details |
| --- | --- | --- |
| Collaborative platform | Support synchronous &amp; asynchronous meeting activities with shared meeting tools such as whiteboard, notes, screen sharing &amp; online meeting recording. | Adobe Connect meeting room for each group available 24/7, provisioned through the University&#39;s learning &amp; teaching system. |
| Collaborative platform | Support synchronous &amp; asynchronous meeting activities with project-oriented tools such as milestones, issues and workflow systems; plus documentation and participation logging. Model representation supported in Markdown (text) and linked Graphviz (diagram) forms. | GitHub repo for each group available 24/7, provisioned through the University&#39;s learning &amp; teaching system. |
| Conceptual scaffolding | Examples, instructions, and webtools for using PlantUML, Graphviz and Markdown. | Instantiated in each repo before the group task commenced, supported through in-class activities. |
| Modelling tools and notation | PlantUML, Graphviz and Markdown languages and tools. | planttext.com;
 gravizo.com;
 plantuml.com;
 marpdown.com;
 plus desktop applications linked from each repo as well as the Learning Management System (LMS) (Canvas). |
| Group regulation tools | Templates for role-based workflow such as facilitation scheduling, meeting notes &amp; reports provided for students to adapt to their context. | Instantiated in each repo before the group task commenced, supported through out-of-class activities. |
| Group social &amp; epistemic tools | Meetlets adapted as templates describing typical collaboration/meeting activities such as ideation, elaboration, refinement and evaluation, and how to instantiate them on GitHub and Adobe Connect provided. | Instantiated in each repo before the group task commenced. |
| Group design tools | Workflows for iterative design and decision making and how to instantiate them on GitHub and Adobe Connect. | Instantiated in each repo before the group task commenced. |
| Group tool scaffolding | Help information, FAQs and (video) tutorials at beginner, intermediate and advanced levels for all the tools and platforms used in the unit. | Set out in the university&#39;s Canvas LMS. |
| Conceptual scaffolding | Step-by-step instructions, disambiguation and elaboration. | Wiki pages added to the class GitHub repo. |
| Just-in-time individual assistance | Personal and individual assistance provided in the Adobe Connect class room. | Drop-in synchronous web-based clinic for all students on a weekly basis. |

All documents and artefacts such as diagrams and notes, including the resources for project and group management, were managed by the teams in their repository and meeting room, with their presence and form persistent so those spaces could reflect the team&#39;s way of working. An example of the resources provided in each group&#39;s GitHub repository is shown at Figure [F].

![](RackMultipart20200724-4-r9jnht_html_24cbceb89f687011.png)

_Figure 1: Example of the activity templates in each group&#39;s GitHub repo._

Meetlet resources are presented here as a series of steps toward an activity, such as &quot;evaluative activities&quot;, with an initial specification of tools and artefacts with which it is conducted - essentially design patterns with a feedback dimension. As an additional domain-related scaffold, these are presented in PlantUML language, which is the format in which the unit group project is developed in. This has the advantage of providing both a graphical and narrative representation of each meetlet, allowing learners to experiment with them, and we expect to modify them as a practice and also a tool. A partial example of a meetlet resource is at Figure [F] with a full example at Appendix [A].

![](RackMultipart20200724-4-r9jnht_html_f1dacaa9f1fbfd94.png)

_Figure 2: Example of meetlet for a brainstorming activity._

An initial screen layout structure which can be modified by learners was also instantiated in each team&#39;s Adobe Connect meeting room. The environment layout and contents can be changed in real time to suit user activities and will stay in the last configuration until modified. At any point in time the meeting host can save a particular layout as a template and give it a name. An example of how the structure and features can be organised is at Figure [F].

![](RackMultipart20200724-4-r9jnht_html_61778eb9fe448766.png)

_Figure 3: Example of Adobe Connect room layout._

### Designed task

Aligned with the unit Learning Outcomes and major assessment task, student groups used PlantUML Activity Diagrams {Object Management Group®, 1997 #7102}to construct models related to learning design. This is a simple and intuitive modelling notation, but there is still tension between what the student wants to express of their mental model, and what can be expressed in the model notation. They were encouraged to use the [graphviz](https://www.graphviz.org/) DOT notation {Ellson, 2018 #7101} for system modelling instead of producing diagrams with a drawing tool, as the plain text format has advantages for editing, reporting and providing system feedback on model validity. An illustration of how the notation provides both a narrative and visual view on the system is at Figure [F].

![](RackMultipart20200724-4-r9jnht_html_e00f626d1d5a5c5a.png)

_Figure 4: Example of the multiple representational format possible using UML._

Work was structured by providing tools and templates including roles that rotate between team members in turn. Each week, the _Facilitator_ prepared and guided the synchronous collaboration (including recording meetings using the web conferencing systems), encouraging full participation, transparent deliberation and shared decision making. The Facilitator was supported with a range of templates for various group activities (meeting agenda, brainstorming, structuring, voting, etc) within the learning environment. The _Knowledge Manager_ (KM) acted as scribe during the synchronous meeting, produced rich meeting records, maintained the organisation of the documents in the repository, and kept notes of experiences with system documentation management and asynchronous deliberation and decision making. The KM also helped the Facilitator to prepare meetings by providing a weekly overview of activities, decisions, and progress. In other words, he or she maintained group memory. The other students in the team took on the role of _domain experts_ with responsibilities for the research that informed their models and the group report.

The design also relied on the lecturer who taught the class and the researcher (me). My main task during the project was to support the students in their function as Facilitator and Knowledge Manager, and with their practical skills in UML modelling, GitHub and other technical areas. This included preparing them for these roles with resources and practice, as well as operational support while they were working in these roles.

During the conduct of the study, I was able to respond to learner feedback about the environment by attending classes and hearing where participants were experiencing problems with either tools or task. To make sure this knowledge was accessible, I created a wiki in the {Shaw, 1978 #8440}whole class GitHub site and the lecturer and/or I addressed each question or problem with a short explanation and links to worked examples and/or video resources. The topics covered were:

- [Marp for slide production](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/Marp-for-slide-production)
- [Can teams assign issues?](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/Can-teams-assign-issues%3F)
- [Week 6: Develop group process - elaboration](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/Week-6%3A-Develop-group-process---elaboration)
- [Component diagrams](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/Component-Diagrams)
- [Sequence Activity Interaction diagrams](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/Sequence-Activity-Interaction-diagrams)
- [Use case diagrams](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/Use-case-Diagrams)
- [Host privilege error](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/host-privilege-error)
- [Merge conflicts](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/merge-conflicts)
- [Meeting documents in folder](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/meeting-documents-in-folder)
- [Adobe Connect meeting recordings](https://github.sydney.edu.au/crli/EDPC5022-2019/wiki/Adobe-Connect-meeting-recordings)

An example response is at Figure [F]

![](RackMultipart20200724-4-r9jnht_html_ac005b9589e59beb.png)

_Figure 5: Screen shot of first part of Week 6 breakdown wiki page._

###


### Data collection in Study 1

It was intended that data be collected at five points during the study. Collections 1-4 would comprise survey responses, collection 5 comprised a range of artefacts and knowledge objects from the designed collaboration environment and tasks.

However, due to a university restructure, the IT infrastructure for these surveys was delayed by four months, meaning data collection did not proceed as planned, and it was necessary to reformulate the study.

_Table 4: Data collection timeline in Study 1._

| **Data Collection** | **Teaching week
 2019 **|** Data** |
| --- | --- | --- |
| **Data Collection 1** | Week 6 (Sem 1) | Survey N=4/9
 What do you think about \&lt;elements\&gt; as collaboration platforms? |
| **Treatment 1** | Week 10 | Peer exchange of insights &amp; practices |
| **Data Collection 2** | Week 13 | GtiHub Interaction data &amp; artefacts |
| **Analysis 1** | Week 0 (Sem 2) -4 | Partial analysis &amp; preparation for Study 2 |

### Data collection 1

Learners were surveyed in Week 6 of session, using an externally provided server as the study server had not been provisioned. While conducting the survey that server became unavailable, data was captured for ten participants. The survey prompt took the form &quot;Your perception of collaboration platforms and how they are similar to or different from each other&quot;. Respondents were given a selection of three _elements_ (the items comprising the system of enquiry) and were asked to differentiate two elements from one, and describe why one is less related to the others. The sequence and combination of elements presented are traditionally mixed so that each element appears with each other element at least once, and no combination of elements appears twice [find ref – think Candy]. In this collection, only three presentations were possible due to infrastructure failure. In all the RepGrid data collections, participants used their own device to complete the survey and did not to my knowledge talk among themselves; not all participants were present in the classroom as the unit is offered in blended mode so the face-to-face participation varied on a week-to-week basis.

The _elements_ presented were:

- Canvas
- Dropbox
- Facebook
- GitHub
- Google Docs/Slides/Draw
- Google Drive
- Slack
- WhatsApp
- The ideal collaboration environment

These elements were selected as they were, in the opinion of the instructor and researcher, systems that were within the diverse cohort&#39;s _range of experience_.

A screen shot of how the elements were presented to the respondent is at Figure [F], along with an indication of how participants supply the constructs (not real data).

![](RackMultipart20200724-4-r9jnht_html_ca33935645499ffa.png)

_Figure 6: Presentation of Elements in the survey for data collection 1_

{Neimeyer, 2002 #8038} calls this _triadic difference,_ different from _dyadic difference_ as there are three rather than two elements, and from both _triadic_ and _dyadic opposites_ as it asks the participant for two contrasting constructs as they relate to that element, rather than one construct and a notional opposite pole which may not be relevant to that element. Once the first three elements had been categorised by the respondent, the remaining elements were presented and the participants rated them along the continuum between the construct poles. Rating element-by-element has been demonstrated to increase the construct system differentiation {Neimeyer, 2002 #8038}, useful here to capture the broadest possible view. A screen shot of how the elements are rated on the new construct is at Figure [F], along with an indication of how participants might rate each element on that construct (not real data).

![](RackMultipart20200724-4-r9jnht_html_bee547d694315d34.png)

_Figure 7: Element/Construct rating method in data collection 1_

### RepGrid overview

Each participant had ten minutes to add two or three constructs. The initial data display for a completed RepGrid is at Figure [F]. Full grid response data is at Appendix [A]. Before addressing specific results, an overview of analytic representations generated for repertory grids is below.

![](RackMultipart20200724-4-r9jnht_html_17a0f586e0309f5e.png)

_Figure 8: Completed RepGrid by participant &quot;yurungi&quot;._

Following {Jankowicz, 2005 #6949} the first step for analysing computer-elicited grids is an &quot;eyeball analysis&quot;. We can see in Figure [F] above that the respondent was in a short length of time able to differentiate the elements in relation to three conceptually different things: _social/goal orientation,__LMS/collaboration_ and _casual/organisational_ use. While manual reversal of poles is possible, generally the left-hand pole = 1 on the continuum and the right hand =5. So in the top row of this raw grid, the respondent has rated _WhatsApp_ and _Facebook_ most closely to the _Social_ pole of the continuum and _GitHub_ and _The ideal collaboration environment_ closest to the _goal_ pole. While the ratings are important, it is also important to note that the constructs can provide insights into what the participant is interested or involved in {Jankowicz, 2005 #6949} and in themselves are a framework of meaning-making about the system of inquiry. However, this data is not very human-readable, despite having significant potential for machine-readability. A particular affordance of the WebGrid application is that users can view their grid in multiple representational formats, to make sense of it in different ways. The grid shown at Figure [F] above is shown in several representations below in Figures [FFF].

#### FOCUS Cluster

The purpose of the FOCUS cluster is to reorganise the grid data in a way that foregrounds the structural relationships between both elements and constructs. The application calculates similarity scores between the first pair of elements (in this case Dropbox and Canvas) on the first construct (in this case social/goal orientation), then the second construct, and then subsequent constructs, working down the two columns. This is repeated for each pair of elements (1 and 3, 1 and 4 and so on) until all elements and constructs have been organised so the most similar are side-by-side. You can see if Figure [F] that the dendrograms for both elements and constructs (the red and blue branch-like lines connected to each element and construct) join to indicate the degree of similarity between ratings. For example, looking at the red lines showing element relationships, both _WhatsApp_ and _Facebook_ and _The ideal collaboration environment_ and _GitHub_ have lines aligning with the &quot;100&quot; marker, indicating that the respondent has rated them 100% the same on each construct. This can be interpreted as showing that the respondent thinks of those elements very similarly in relation to the constructs presented. If we look at the blue lines showing construct relationships, the first connection between lines is aligned around the &quot;70&quot; marker, indicating that there is about a 70% similarity in responses for the _Social/goal_ construct and the _Casual/organisational_ constructs, showing more similarity between these constructs for the respondent than between either of those and the _LMS/Collaboration_ construct. The ratings are shaded to show high (dark), medium (grey) and low (white) numbers, indicating proximity to the poles. For example, _Dropbox, Facebook, WhatsApp_ and _Google Drive_ are at the _casual_ pole, with ratings of 1, whereas _Canvas_ and _Slack_ are at the _organisational_ pole with ratings of 5, and the other elements somewhere in between. This can be interpreted as the strength of construal, with ratings at the poles very definite and those in the middle less so. There is vigorous debate about the middle numbers in a grid, which will be elaborated in the Discussion chapter of this thesis.

![](RackMultipart20200724-4-r9jnht_html_7dd33033aafda9fd.png)

_Figure 9: Completed RepGrid by participant &quot;yurungi&quot; represented as a focus cluster._

#### PrinGrid

The representation at Figure [F] is the same data, represented in the form of a Principal Components Analysis (PCA). The black x and y axis lines represent the two components having the most different patterns in the dataset. The blue lines are positioned at an angle to the axes representing the extent to which each construct matches the component pattern, and at a length that indicates how variable the ratings on that construct are. In Figure [F], for example, both _goal_ and _organisational use_ are close to the horizontal axis, indicating that the pattern of responses on those constructs has the highest variance (58.6%), and _collaboration_ is almost adjacent to the vertical axis, the second-highest pattern variance (31.0%). The more similar the rating across constructs, the smaller the angle between the lines. So, the first component represents the constructs _goal_ and _organisational_ very well, but _collaboration_ not well, and the second component represents the construct _collaboration_ well but not _goal_ or _organisational._ The elements are plotted in red text in relation to the constructs in a space that uses the first component as x-axis and the second component as y-axis. This means distance between elements reflects the similarity of ratings across all the constructs, which we can see by going back to the Focus cluster, which shows _goal_ and _organisational_ with the most similar ratings, and _collaboration_ with different ratings. The most important elements in the grid are those furthest from the origin, as they dictate the direction and scale of the plot in which the other elements are placed {Slater, 1965 #1795}.

![](RackMultipart20200724-4-r9jnht_html_b9d54d33024a13de.png)

_Figure 10: Completed RepGrid by participant &quot;yurungi&quot; represented as a Principal Components Analysis plot._

#### Crossplot

A crossplot may be used when two constructs are considered together, reducing the complexity of the graph. In Figure [F] we can see if our two most important constructs are _Social/goal_ and _casual_/_organisational_, those elements that scored higher on _goal_ are on the right hand side of the vertical axis toward either the _casual/organisational_ pole and we can see the participant has scored _The ideal collaboration environment_ furthest toward the _goal_ axis and exactly half way between _casual/organisational_. If we then take out that differentiation and reorganise the data so that our most important constructs are _Social/goal_ and _LMS/Collaboration_, we can see _The ideal collaboration environment_ still at the _goal_ end, and also now at the _Collaboration_ end. So our participant strongly associates _Collaboration_ and _goal_ orientation with their ideal collaboration space.

![](RackMultipart20200724-4-r9jnht_html_838e8a6f552dff58.png)

_Figure 11: Completed RepGrid by participant &quot;yurungi&quot; represented as a crossplot graph using the &quot;Social/goal&quot; and &quot;casual/organisational&quot; constructs._

![](RackMultipart20200724-4-r9jnht_html_81399480000d1f19.png)

_Figure 12: Completed RepGrid by participant &quot;yurungi&quot; represented as a crossplot graph using the &quot;LMS/Collaboration&quot; and &quot;Social/goal&quot; constructs._

### Data collection 1 - Findings

|
- Individual and group perceptions – How do you think of these collaboration technologies: Canvas; Dropbox; Facebook; GitHub; Google Docs/Slides/Draw; Google Drive; Slack; WhatsApp; The ideal collaboration environment
 |
| --- |

While the survey was only conducted once, Repertory Grid Technique still provides rich data for even small groups of students, valuable in making design decisions {Aditomo, 2011 #6600}. Of the nine participants, two generated three constructs and seven two constructs in their triadic elicitation process with elements supplied as described in Method above. As shown in Figures [FF] above, there are some results that can be identified simply by &quot;eyeball analysis&quot;. To see how accurately that reflects the group results, we will combine methods provided by {Bell, 2005 #6574@@author-year}, {Fromm, 2003 #6430@@author-year}, {van de Kerkhof, #8433@@author-year}, {Aditomo, 2011 #6600@@author-year} and {Scholz, 2015 #7187@@author-year}. Raw participant responses are at Figure [F] below.

![](RackMultipart20200724-4-r9jnht_html_7b7bde397774504e.png)

_Figure 13: Raw responses. Dataset 5022-1-collabtech-deid-2 -\&gt; Composite -\&gt; Display. \&lt;5022-1-collabtech-raw.rnet\&gt;_

#### Cluster analysis to identify thematic groupings

Cluster analysis is one of the traditional methods to interpret RepGrid data{Bell, 2005 #6574@@author-year}. As our survey window was limited by time, we did not have the opportunity to explore whether we reached a true saturation point where no new constructs could be elicited {van de Kerkhof, #8433}. However, using quick thematic &quot;eyeball analysis&quot;, some constructs are more represented than others at a higher level of abstraction; about the locus and intent of the use. While this is interesting in itself and will be discussed later, here we look at the value of the Repertory Grid Technique as a quantitative method capable of dealing with qualitative data. Table [T] below shows my researcher interpretation of respondent constructs. This is based on my limited worldview, unconscious bias and way of meaning-making; it takes quite a bit of time, and without a second data collection can not be considered to be true of the participants&#39; way of construing reality. However, using Personal Construct Theory [Kelly etc] and the statistical and representation models developed by Shaw, Gaines and others [{Shaw, 1978 #8440}for FOCUS, {Slater, 1964 #8044}for PrinGrid, long list here later], we can get some insight into how valid these assumptions are.

_Table 5: How thematic grouping might be used to interpret survey responses._

| **Constructs provided by participants** | **Manual clustering** |
| --- | --- |
| **personal / professional** | Private decision to use – Org decision to use |
| **casual/organisational use** |
| **social/goal** | Social purpose – Org purpose |
| **social manager/work manager** |
| **social media/collaborative** | Collaboration purpose – Communication purpose |
| **chat/collaborative** |
| **communication/collaboration** |
| **chat/document sharing** | File/document sharing – Communication purpose |
| **communication/file sharing** |
| **social media/sharing docs** |
| **filesharing /social chat** |
| **repository/communication** |
| **social /document** |
| **LMS/collaboration** | Educational purpose – information exchange purpose |
| **learning/exchange orientation** |
| **learning/information exchange** |
| **work collaboration/file collaboration** | Doing things together – uploading things apart |
| **one way /shared communication** |
 |
| **code/easy to navigate** |
 |
| **models/communication** |
 |

We have seen at [Figure F] how the WebGrid FOCUS clustered representation for a single grid &quot;yurungi&quot; where the high matches in responses for the _Social/goal_ and _Casual/organisation_ constructs indicate they are thought of more similarly than _LMS/Collaboration_, and at Table [T] how a researcher might interpret multiple responses in a manual thematic analysis process. Figure [F] shows a FOCUS clustered representation of all constructs provided by all participants after removing oneempty construct (_A/B_), and then two constructs that had three &quot;?&quot; ratings (_communication/file sharing_ and _learning oriented/exchange oriented_). There is one duplicate (_social media/collaborative environment)_, but the same participant rated them each differently. At the moment, that duplicate will stay in place as we can&#39;t tell which is intended and it may not matter very much in this initial phase. The remaining 4 &quot;?&quot; ratings are treated as being at level 3=neutral for the purpose of this cluster analysis representation {Gaines, 2018 #8045}.[Shall I make the examples look a bit nicer from here on in? Changed to Lato 12, white floral bg and brighter green data – should it be larger?, example over page – deliberate temporary duplication.]

![](RackMultipart20200724-4-r9jnht_html_96e6de9204de3a3.png)

_Figure 14: FOCUS cluster of participant responses after empty constructs removed. Dataset 5022-1-collabtech-deid-2 -\&gt; Composite -\&gt; Focus Cluster. \&lt;5022-1-collabtech-focus.rnet\&gt;_

If we look to see how the participants rate constructs around _The ideal collaboration platform_, looking up from that element in red along the bottom of the matrix, it is notable that there are no ratings of one or two, and there are two &quot;?&quot; which for the participants means &quot;I&#39;m not sure&#39;, and for the analysis &quot;I&#39;m neutral&quot;. The same participant &quot;bunyarinarin&quot; scored both &quot;?&quot;s in relation to the constructs _collaboration/communication_ and _repository/communication_, so it is likely they have at this time ambiguous thinking about how that ideal environment is placed between these three ideas **collaboration** , **communication** , **repository** _._ Over all 23 constructs, the ideal environment has 13 scores of 3 out of the 30 total mid-point ratings on the grid. First conclusion is that the ideal environment is not well-defined by the participants, but lies somewhere within – between – amidst - the framework of these constructs. We do get some stronger preferences with scores of 4-5 for goal orientation, collaboration, share and information exchange, which could be read as that space having a sense of purpose as well as shared representational infrastructure. In order of proximity following the ideal are _GitHub_, _Google Slides/Draw_, _Google Drive_ and _Dropbox_, _Slack_, _WhatsApp_ and _Facebook_. _Google Drive_ and _Dropbox_ are the most similarly-construed elements and _Canvas_ and _Facebook_ the most differently-construed.

We can also remove unique constructs to get more condensed data on those most similar. Taking away the &quot;standalone&quot; classifications of work collaboration/file collaboration and communication from a single participant (13, 14), one way /shared communication (18), code/easy to navigate (16), and models/communication (17), what is reflected in the graph at Figure [F]? Table [T] shows how RepGrid aligned the thematic coding based on purely quantitative data.

![](RackMultipart20200724-4-r9jnht_html_347a7cc31bc7f36a.png)

_Figure 15: FOCUS cluster of responses with single-occurrence concepts removed. Dataset 5022-1-collabtech-composite-pruned-2.rgrid -\&gt; Composite -\&gt; Focus cluster \&lt;5022-1-collabtech-focus-pruned-2.rnet\&gt;_

Looking down the rows, the first cluster starts with two almost identical constructs from participant &quot;wibung&quot; (15 ,16), and subsequently &quot;binit (3,1) and &quot;budhawa&quot; (4,6), creating a subset of constructs related to document/file sharing versus chat from constructs 16 down to 6. What if we take all these out of the set as cancelling each other out? And here we can also remote the duplicates identified before, _Social media_/_Collaborative environment_ from &quot;dyaramak&quot; as it too is less relevant at this point. Figure [F] shows the result.

![](RackMultipart20200724-4-r9jnht_html_746ed7620cf6e28f.png)

_Figure 16: FOCUS cluster of responses with duplicate-construct pole and ratings removed. Dataset 5022-1-collabtech-composite-pruned-3 -\&gt; Focus cluster \&lt;5022-1-collabtech-focus-pruned-3.rnet\&gt;_

The clusters in both construct and element dimensions are now clear. The first cluster is between constructs 1, 7, 9. Each of the three rows has the ideal collaboration environment scoring at the **work/goal end of the social/work-related continua** and about **sharing**. It is in turn contained by constructs 3, 4 ,2, which show the **tensions between** _ **social** _ **and** _ **work** _ **,** _ **communication** _ **&amp;** _ **collaboration** _tool, and at the _ **collaboration** _ **-oriented** pole on the _entertainment_ construct. In the lower half of the construct rows, constructs 8 and 10 rate the ideal at **the opposite pole from** _ **platform oriented to learning** _ **and** _ **LMS** _ **.** The larger cluster completes then with the remaining responses - almost certain that it should be **collaborative** rather than **chat** but again unpreferenced between **casual/organisational us** e and **communication versus file/information exchange**. The elements are also clearly clustered, with 4, 5, 6, 2 showing substantially similar ratings (around 80%). The next cluster up is _The ideal collaboration environment_ and _Canvas_, then _Slack, WhatsApp_ and _Facebook._ What this highlights is that while the ideal is not an LMS or learning-oriented environment on those construct ratings, _Canvas_ has closer ratings to the ideal that _WhatsApp_, _Facebook_ or _Slack_. Figure [F] shows how while I categorised responses by the intention or purpose of use, the quantitative chunking of responses was more about features and functionality of the different environments.

| **Constructs provided by participants** | **Manual Coding** | **FOCUS clusters** |
| --- | --- | --- |
| **personal / professional** | Private decision to use – Org decision to use | Work/goal-oriented |
| --- | --- | --- |
| **casual/organisational use** |
 |
| **social/goal** | Social purpose – Org purpose |
 |
| **social manager/
 work manager** | Somewhere in the middle of social v work use |
| --- | --- |
| **chat/collaborative** | Collaboration purpose – Communication purpose | Collaboration involving communication |
| --- | --- | --- |
| **communication/
 collaboration** | File/document sharing – Communication purpose | Somewhere between casual and organisational use |
| --- | --- | --- |
| **social media/sharing docs** |
 |
 |
| --- | --- | --- |
| **LMS/collaboration** | Educational purpose – information exchange purpose | Not about learning |
| --- | --- | --- |
| **learning/information exchange** |
 |
 |
| --- | --- | --- |
|
 |
 | Communication &amp; Information exchange |
| --- | --- | --- |

_Figure 17: Coloured cells indicate the clusters in RepGrid next to the human-coded groups._

Although not identical to the manual coding sets, this result is simpler, can be run much faster, and provides insight into the gap between what I as the researcher might think I see, and what is actually there. Reviewing the FOCUS graph knowing this, I can assess potential ideal environments against these slightly different, but more authentic, evaluative ideas: casual versus work/goal orientation (should be more work/goal), social/work use (somewhere in the middle), chat/filesharing (also somewhere in the middle). Something useful I took from this result was that the idea of &quot;formality&quot; or an &quot;official&quot; system to work in was less important than the use of the system being relevant to the goal of the collaborative work. At this point, RepGrid might seem to offer only a faster, slimmed-down alternative to qualitative analysis of a survey, which is a valid point. Using the same data, and now having some idea of the key constructs that are associated with collaborative environments, what can we learn from this single survey that can inform our next designed environment?

#### Principal Components Analysis

As part of a British Medical Research Council initiative in 1965, Patrick Slater developed a computerised grid analysis application that could generate two- and three-dimensional representations of both constructs and elements in spatial relationship to each other {Slater, 1965 #1795}. This form of Principal Components Analysis is also a traditional format {Bell, 2005 #6574@@author-year}, and implemented in the WebGrid/RepPlus program as the &quot;PrinGrid&quot; seen at Figure [F]. As this data has been manually reviewed and clustered, then automatically clustered, reviewed and re-reviewed it has already undergone a type of component analysis. We have found patterns in the data which are identical and removed them to focus more on constructs that have different responses. But it does not mean they are unimportant to the overall analysis, so in this part of the analysis we will reinstate them. We are returning the dataset to that shown in Figure [F14] Dataset 5022-1-collabtech-deid-2 -\&gt; Composite -\&gt; Focus Cluster. \&lt;5022-1-collabtech-focus.rnet\&gt;, now represented in Figure [F] in PrinGrid format [Shaw citation here].

![](RackMultipart20200724-4-r9jnht_html_2f0bfe2dd46b4d7d.png)

_Figure 18: Original dataset after empty constructs removed in PrinGrid format. Data Dataset 5022-1-collabtech-deid-2 -\&gt; Composite -\&gt; PrinGrid \&lt;5022-1-collabtech-pringrid-pruned.rnet\&gt;_

We can again see visual clustering clearly, but in this representation the elements are positioned among the constructs in an x-y matrix with the proximity to the axes indicative of the highest and second-highest variance among pattern responses. Here, aligned more to the x-axis, 58.4% of variance in response patterns is around the construals of **communication versus file/information exchange** and **collaboration versus chat** that was also the highest-level cluster in our previous view. Looking back at the data, these constructs had ratings across the row starting with low, then undecided or mid-level responses then high or ending with a mid-level. The second-highest variance is represented in relation to the y-axis, only 15.5% and less for the subsequent components. A high level of variance in response patterns shows individually different ways of thinking about the elements in relation to the constructs, which is what I am interested in.

An &quot;eyeball analysis&quot; can see that the elements are in the same clusters as the final FOCUS grid at Figure [F16]. _Slack_, a long way away from ideal, _Facebook_ and _WhatsApp_ close together and proximate to the _social_ and _personal_ constructs. _Dropbox, Google Docs/Slides/Draw_ and _Google Drive_ are firmly sited in _document sharing_ and, also, we can see here, to the _collaborative environments_ pole contrasting with _social/chat._ This also shows _GitHub_ and _Canvas_ positioned closest to _The ideal collaboration environment_, a little different to Figure [F16], but this PrinGrid was generated without the extra steps of removing constructs from the original dataset, while returning substantially similar results in an easy-to-read form. From a CSCL perspective, this also tells me that the participants are unlikely to know the features of the _Slack_ platform, as it is a frequently-used professional tool that integrates with a range of file sharing, communication and other collaborative platforms, such as Office365 and the Google suite of applications {Slack Technologies, 2020 #8048}. Without this survey, I would have needed to review all the class activity logs, work products, communications, meeting recordings and reflections to find this out. I might not have been looking for it if I could not easily generate the PrinGrid representation. What this means for the next iteration of design will be discussed below.

-multidimensional unfolding to represent the relationships between constructs and elements spatially {Fromm, 2003 #6430}.

#### Exploratory factor analysis to examine relationships between elements and constructs

We saw at Figure [F14] how ill-defined _The ideal collaboration environment_ is by participants, but there was preference for goal orientation, collaboration, share and information exchange. If we want to reduce the number of parameters we can begin with the nine components found in the PCA above. A linear mixed (effects are both fixed and random) model approach, which can still provide useful data in the absence of repeated-measures is appropriate to a site where participants are themselves the random effect {Heckmann, 2015 #7391}. Found a backdoor to the analysis software!!! [http://www.onair.openrepgrid.org/](http://www.onair.openrepgrid.org/)

\&lt;I am leaving this as I&#39;ve spent two days on Factor Analysis and still don&#39;t think I understand how to do it.\&gt;

I know this scree plot tells me there is a significant inflection point at 1.8 and shows another around 4. And I&#39;m guessing that I manually chunk those component clusters but I still don&#39;t really get what I&#39;m looking for. And if that makes 3, that is consistent with the frequency analysis.

I know that the goal is to find correlation, eliminating the unique (in this case individual students difference) factor, keeping the estimated factor and inferring causality.

- exploratory factor analysis to examine relationships between elements and constructs {Aditomo, 2011 #6600}

![](RackMultipart20200724-4-r9jnht_html_d6309ccf114675d1.png)

_Figure 19: Scree plot of Principal Components Analysis._

#### Frequency analysis

Following {van de Kerkhof, #8433}, I counted the constructs generated, using the dataset without empty responses but (Figure [F14]) this time deleting the duplicate construct continuum _Social media/collaborative environment_ to make 23 elicited constructs across all participants. We have seen how closely some constructs resemble each other, with only slightly different wording, Again following {van de Kerkhof, #8433} I tested for construct saturation by calculating how many new constructs each participant added. As the participants completed the grid all at the same time, it is not possible to strictly assign sequence to the constructs so Figure [F] below shows new constructs by alphabetical name order. As participants were still adding one new construct at the end of the survey I can not claim we reached saturation. However, given the flattening of the graph and low number of novel constructs, with a larger set of participants and a longer survey timeframe it would be achievable.

![](RackMultipart20200724-4-r9jnht_html_368e0e2bdfa904b6.gif)

_Figure 20: New constructs added by participant. \&lt;Constructs tab\&gt; /Users/eb/Dropbox/Documents/-Fed/Termites-and-Butterflies/Data/5022-S1-2019/EDPC5022-survey-data-20191116.xlsx_

There were differences both within constructs and across teams in construct originality. Team A contributed the same number of constructs as Team B, with three participants generating one unique construct, where in Team B a single participant generated two unique constructs. From Team C only one team member contributed constructs with one being unique, and a single participant that did not complete the course contributed three non-unique constructs. Table [T] below shows the different contributions by team and topic.

_Table 6: Breakdown of constructs by team and originality/uniqueness._

| Construct | Team A | Team B | Team C | No team | Total |
| --- | --- | --- | --- | --- | --- |
| Code versus Easy to navigate |
 | murradjulbi |
 |
 | 1 |
| Collab not Chat | dyaramakdyuralya |
 |
 | bunyarinarin | 3 |
| Collab not LMS | yurungi |
 |
 |
 | 1 |
| Collab not social | dyaramak |
 |
 |
 | 1 |
| DE not chat | dyuralya | binit | wibung (x2) | bunyarinarin | 4 |
| DE not LMS |
 |
 | wibung |
 | 1 |
| DE not social |
 | binit budhawa (x3)murradjulbi |
 |
 | 3 |
| Models versus Communication tools |
 | murradjulbi |
 |
 | 1 |
| One-way communication / Share oriented | warin |
 |
 |
 | 1 |
| Work/Goal | yurungi (x2) | binit |
 | bunyarinarin | 3 |
| Total by team | 7 | 7 | 2 | 3 |
 |
| Total by participant | dyaramak 2 dyuralya 2 yurungi 2warin 1 | murradjulbi 3 binit 3budhawa 1 | wibung 2 | bunyarinarin 3 |
 |
| By uniqueness | dyaramak 1 yurungi 1warin 1 | murradjulbi 2 | wibung 1 |
 |
 |

The responses fell overwhelmingly into two categories, as shown in Figure [F] below. Eleven (around 48%) of the responses construed a collaboration environment as being a document or file sharing or exchange system and not, in order, chat, social or a learning management system. The next most construed set of poles were that the environment was collaborative and not, in order, chat, social, or a learning management system. It was then seen as a work- or goal-related environment as opposed to casual or personal, then the remaining three constructs that only one person mentioned related to interface and purposeful use of the environment. This last could be loosely considered to belong to the Work/Goal-oriented construct. Looking back at Figure [F18], this mirrors the PrinGrid results, with collaboration opposite chat and social use, and document sharing opposite communication tools. This is interesting, and important, for the design of learning environments as it might indicate that collaboration is seen as primarily about documents and knowledge objects, and not about the communicative processes that underpin learning from a constructionist perspective.

![](RackMultipart20200724-4-r9jnht_html_52ec0dca8bf76477.png)

_Figure 21: All constructs by topic. \&lt;Constructs\&gt; tab of EDPC5022-survey-data-20191116.xlsx in /Users/eb/Dropbox/Documents/-Fed/Termites-and-Butterflies/Data/5022-S1-2019_

{van de Kerkhof, #8433} used the least-squares method and a two-dimensional biplot to find the three most divergent visions across their dataset. We have seen the PrinGrid at Figure [F] showing all elements and all constructs, and identified that some constructs are mentioned more frequently than others. We can find deeper information using a similar biplot for our study data by selecting only two dimensions for analysis. {Fromm, 2003 #6430} makes the point about education, if we do not know the constructions that learners will find meaningful, how do we know what output to look for? In Figure [F], we cross-plot two almost identical constructs contributed by two participants, showing that they both construe collaboration environments, generally, and almost universally, to be work-related and not social. But from the previous analysis, we knew this already.

![](RackMultipart20200724-4-r9jnht_html_aa2bbfc020c84d2e.png)

_Figure 22: Two-dimensional plot of elements on the constructs of two participants around work-social use. Data Dataset 5022-1-collabtech-composite-pruned-\&gt; Crossplot with personal-professional at x and can be casual use – organisational /work use at y-\&gt; Crossplot_

### Data collection 1 – comparing teams \&lt;move to discussion?\&gt;

At Table [T5], we can see that while the participants who went on to form Teams A and B made three and two unique contributions respectively, and 7 total contributions, and another participant who did not complete the unit made 3, the participant that went on to Team C made only one unique response and one response made by both other teams (and the non-team participant). Given the sudden unavailability of the survey platform, this might simply indicate that participant took longer than others to access the survey and enter their responses. Alternatively, it might indicate the participant had difficulty construing a collaboration environment at a more nuanced level than their simple binary contributions of document exchange versus chat, and document exchange versus LMS. The frequency with which these kinds of distinctions were made shows it was difficult for all participants to generate more sophisticated constructs. Figure [F] represents the most commonly-construed elements and constructs, across all grids, as probability of similar construing between participants. The arrow from &quot;wibung&quot; (Team C) to &quot;binit&quot; (Team C) shows that the average over all the constructs in &quot;binit&quot;&#39;s grid of the matches of the best-matched constructs in &quot;wibung&quot;&#39;s grid is higher than 80%, showing that &quot;wibung&quot; should be able to understand &quot;binit&quot;&#39;s, constructions, and other arrows also show a high match with &quot;budhawa&quot; (Team B). However, it is not until we reduce the match threshold to under half that all participants have an adequate similarity of construing to be considered understandable to every participant, illustrated at Figure [F] with specific match percentages shown. This might indicate participants have differential constructs around collaboration technology within their teams. If this is the case, it might also be identified in the analysis of Data Collection 2.

![](RackMultipart20200724-4-r9jnht_html_bc6c7eb05420b9cb.png)

![](RackMultipart20200724-4-r9jnht_html_3eed8fe4be52d515.png)

[When time permits re-do the socio but in teams, although numbers really too small]

[Next step is to do some CA on the team meeting recordings, looking to see whether team and/or individual contributions in the RepGrid exercise could be indicative of behaviour in the subsequent tasks.]

Should I include these table or leave out? Or Appendicise?

_Table 7: Similarity in construct and element rating from highest to lowest._

| % match | from |
 |
 |
 |
| --- | --- | --- | --- | --- |
| 86.11 | dyuralya | budhawa | A | B |
| 86.11 | binit | budhawa | B | B |
| 85.19 | budhawa | binit | B | B |
| 84.26 | dyuralya | binit | A | B |
| 81.48 | wibung | budhawa | C | B |
| 81.48 | wibung | binit | C | B |
| 80.56 | murradjulbi | binit | B | B |
| 78.7 | budhawa | wibung | B | C |
| 78.7 | dyuralya | wibung | A | C |
| 77.78 | dyaramak | budhawa | A | B |
| 77.78 | budhawa | bunyarinarin | B | check |
| 77.78 | budhawa | dyaramak | B | A |
| 76.85 | budhawa | dyuralya | B | A |
| 76.85 | binit | wibung | B | C |
| 76.85 | yurungi | murradjulbi | A | B |
| 76.85 | binit | dyuralya | B | A |
| 76.85 | bunyarinarin | budhawa | check | B |
| 76.85 | murradjulbi | budhawa | B | B |
| 75.93 | bunyarinarin | binit | check | B |
| 75 | dyuralya | bunyarinarin | A | check |
| 75 | murradjulbi | yurungi | B | A |
| 75 | binit | bunyarinarin | B | check |
| 75 | dyuralya | dyaramak | A | A |
| 75 | murradjulbi | wibung | B | C |
| 75 | murradjulbi | bunyarinarin | B | check |
| 74.07 | wibung | yurungi | C | A |
| 73.15 | dyaramak | bunyarinarin | A | check |
| 73.15 | murradjulbi | dyuralya | B | A |
| 73.15 | yurungi | wibung | A | C |
| 73.15 | bunyarinarin | dyuralya | check | A |
| 73.15 | wibung | bunyarinarin | C | check |
| 73.15 | wibung | murradjulbi | C | B |
| 73.15 | yurungi | budhawa | A | B |
| 72.22 | yurungi | binit | A | B |
| 72.22 | wibung | dyaramak | C | A |
| 71.3 | binit | dyaramak | B | A |
| 71.3 | budhawa | yurungi | B | A |
| 71.3 | budhawa | murradjulbi | B | B |
| 71.3 | bunyarinarin | dyaramak | check | A |
| 70.37 | wibung | dyuralya | C | A |
| 70.37 | dyuralya | murradjulbi | A | B |
| 70.37 | dyaramak | wibung | A | C |
| 70.37 | dyaramak | binit | A | B |
| 70.37 | warin | yurungi | A | A |
| 69.44 | binit | yurungi | B | A |
| 69.44 | binit | murradjulbi | B | B |
| 69.44 | yurungi | dyaramak | A | A |
| 69.44 | bunyarinarin | wibung | check | C |
| 69.44 | dyaramak | murradjulbi | A | B |
| 69.44 | murradjulbi | dyaramak | B | A |
| 68.52 | warin | wibung | A | C |
| 67.59 | dyuralya | yurungi | A | A |
| 67.59 | dyaramak | dyuralya | A | A |
| 67.59 | yurungi | bunyarinarin | A | check |
| 67.59 | yurungi | dyuralya | A | A |
| 66.67 | dyaramak | yurungi | A | A |
| 64.81 | bunyarinarin | murradjulbi | check | B |
| 60.19 | warin | murradjulbi | A | B |
| 59.26 | bunyarinarin | yurungi | check | A |
| 58.33 | warin | binit | A | B |
| 57.41 | warin | budhawa | A | B |
| 56.48 | warin | dyaramak | A | A |
| 55.56 | yurungi | warin | A | A |
| 54.63 | warin | bunyarinarin | A | check |
| 52.78 | wibung | warin | C | A |
| 50.93 | warin | dyuralya | A | A |
| 48.15 | bunyarinarin | warin | check | A |
| 48.15 | binit | warin | B | A |
| 47.22 | murradjulbi | warin | B | A |
| 45.37 | budhawa | warin | B | A |
| 44.44 | dyuralya | warin | A | A |
| 38.89 | dyaramak | warin | A | A |

_Table 8: Table of all contributions and theme chunking with track of who provided which construct._

| **original construct** | **who** | **team** | **larger theme** | **new constructs** | **Cumulative total** |
| --- | --- | --- | --- | --- | --- |
| It&#39;s a filesharing platform — It&#39;s for socialising | binit | B | Document exchange versus social |
 |
 |
| It&#39;s a filesharing platform — You can use these apps for chatting | binit | B | Document exchange versus chat/communication |
 |
 |
| It&#39;s for personal use — It&#39;s for professional work or school | binit | B | Work/goal-oriented | **3** | **3** |
| Social manager — document suppository | budhawa | B | Document exchange versus social |
 |
 |
| social manager — work manager | budhawa | B | Document exchange versus social |
 |
 |
| social media — document suppository | budhawa | B | Document exchange versus social | **0** | **3** |
| collaboration tool — communication tools | bunyarinarin | - | Collaboration versus chat/communication |
 |
 |
| Communication and entertainment approach — Collaboration oriented | bunyarinarin | - | Work/goal-oriented |
 |
 |
| repository — communication tools | bunyarinarin | - | Document exchange versus chat/communication | **1** | **4** |
| Chat oriented — collaborative environment | dyaramak | A | Collaboration versus chat/communication |
 |
 |
| Social media — collaborative environment | dyaramak | A | Collaboration versus social |
 |
 |
| Communication — File collaboration | dyuralya | A | Document exchange versus chat/communication |
 |
 |
| Work Collaboration — Communication | dyuralya | A | Collaboration versus chat/communication | **1** | **5** |
| code — easy to navigate | murradjulbi | B | unique construct |
 |
 |
| models — communication tools | murradjulbi | B | unique construct |
 |
 |
| Social media tool — sharing documents (murradjulbi: I actually think all 3 a | murradjulbi | B | Document exchange versus social | **2** | **7** |
| One way communication — Share oriented | warin | A | unique construct | **1** | **8** |
| chat oriented — document exchange | wibung | C | Document exchange versus chat/communication |
 |
 |
| platform is oriented to learning — for exchanging information | wibung | C | File exchange versus learning system |
 |
 |
| sharing via chat — document sharing | wibung | C | Document exchange versus chat/communication | **1** | **9** |
| can be casual use — organisational/work use | yurungi | A | Work/goal-oriented |
 |
 |
| LMS — Collaboration | yurungi | A | Collaboration versus learning system |
 |
 |
| Social — goal | yurungi | A | Work/goal-oriented | **1** | **10** |

### Data collection 2

Data Collection 2 took place after formal submission of the group project product/s, and comprised electronic artefacts documenting the team meetings and activity in the GitHub environment as well as reflective essays. A summary of data collection 2 is at Table [T].

\&lt;consider Plenary recordings as well? only if I have time \&gt;

| **Type of data** | **Size** |
| --- | --- |
| **Meeting recordings - Team A** | 18 recordings – 7 fails/empty
 Total time: **4:50:23** |
| **Meeting recordings - Team B** | 9 recordings – 1 fail/empty
 Total time: **5:17:07** |
| **Meeting recordings - Team C** | 7 recordings – 0 fail/empty
 Total time: **6:46:55** |
| **GitHub site content – Team A** | Total Files: 90
 Total Lines of Code: 6264
 Total Commits: 121 |
| **GitHub site content – Team B** | Total Files: 84
 Total Lines of Code: 5803
 Total Commits: 96 |
| **GitHub site content – Team C** | Total Files: 85
 Total Lines of Code: 5751
 Total Commits: 104 |
| **Final Reflection essays** | 5 essays ≅ 8500 words
17 essays ≅ 26000 words |

##


### Data collection 2 – Findings

Design decisions from this study informed the collaboration environment design of Study 2. A

summary of in-session emergent challenges is discussed later in this section.

\&lt;to do\&gt;

#### 1. Identify key moments – lead to relevant development of the object

_Interactions /events as unit of analysis_
 - qualitative content analysis of the interaction data and knowledge objects to identify sequences that lead to productive interactions and key moments in relation to the knowledge objects {Damşa, 2016 #9087}.

_Interactions / participation as unit of analysis_
 - qualitative content analysis of the interaction data to establish whether there is an association between level and extent of participation and productive interactions in relation to the knowledge object ;{Panadero, 2015 #9211;Damşa, 2016 #9087;Isohätälä, 2017 #9208}.

#### 2. Map uptake of concepts

_Interactions / Key concepts as units of analysis_

- qualitative content analysis of the interaction data and knowledge objects to identify the process by which key concepts are introduced and are maintained or discarded over the course of development of the knowledge objects {Damşa, 2016 #9087}

#### 3. Performance level – comparing elaboration of group knowledge object (and regulation objects and processes)

_Interactions /knowledge objects as unit of analysis_
 - qualitative content analysis of the final group product and interaction data to identify regulative objects and processes [including peer knowledge objects] that are associated with a more elaborate, coherent, or complex knowledge object {Damşa, 2016 #9087;Järvelä, 2013 #9230}.

#### 4. Map use of environment affordances – how are they instrumentalised differently from the intended design?

_Interactions / environment use as unit of analysis_
 - qualitative content analysis of the interaction data to establish how the learners used the collaborative environment [including peer knowledge objects] and how that might be different to the design intention {Lonchamp, 2012 #7683}

_Table 9: Design Decisions carried forward to Study 2_

| **challenge** | **response** |
| --- | --- |
| busy students | assessment task format design to not overload at end |
| not expecting group task | provided with resources for group interactions |
| not expecting UML modelling | provided with resources for UML modelling |
| late census | impacted those in groups with non-continuing students more than others |
| level of abstraction hard to rote learn | activities designed to build up gradually |
| not expecting to share work | more learning [potentially] and makes it very hard to contract cheat or plagiarise [definitely] |

##


